import os
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
import torchvision.transforms.functional as F
from torch.utils.data import DataLoader, WeightedRandomSampler
from tqdm import tqdm
import mlflow
import random
from PIL import Image
import numpy as np

class Cutout(object):
    def __init__(self, size=32):
        self.size = size

    def __call__(self, img):
        # ì´ë¯¸ì§€ê°€ PIL ì´ë¯¸ì§€ì¸ ê²½ìš° í…ì„œë¡œ ë³€í™˜
        if not torch.is_tensor(img):
            img = F.to_tensor(img)
            tensor_converted = True
        else:
            tensor_converted = False
            
        h, w = img.shape[1], img.shape[2]
        y = random.randint(0, h - self.size)
        x = random.randint(0, w - self.size)
        img[:, y:y+self.size, x:x+self.size] = 0
        
        # ì›ëž˜ PIL ì´ë¯¸ì§€ì˜€ë‹¤ë©´ ë‹¤ì‹œ PILë¡œ ë³€í™˜
        if tensor_converted:
            img = F.to_pil_image(img)
        return img

def parse_args():
    parser = argparse.ArgumentParser()
    
    # SageMaker ê¸°ë³¸ ì¸ìž
    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
    parser.add_argument('--val', type=str, default=os.environ.get('SM_CHANNEL_VAL'))
    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    parser.add_argument('--epochs', type=int, default=20)
    parser.add_argument('--batch-size', type=int, default=32)
    parser.add_argument('--learning-rate', type=float, default=0.0001)
    parser.add_argument('--model-type', type=str, default='resnet50')
    parser.add_argument('--num-classes', type=int, default=3)
    parser.add_argument('--patience', type=int, default=5)
    
    return parser.parse_args()


def get_transforms():
    data_transforms = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    train_transforms = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomResizedCrop(224, scale=(0.9, 1.1)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(15),
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
        transforms.RandomPerspective(distortion_scale=0.2, p=0.2),  # ðŸ”¹ ì¶”ê°€
        transforms.ColorJitter(brightness=0.2, contrast=0.2),       # ðŸ”¹ contrast + brightness
        transforms.RandomApply([Cutout(size=32)], p=0.3),
        transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0))], p=0.2),
        transforms.RandomApply([transforms.RandomAdjustSharpness(sharpness_factor=2)], p=0.3),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

    

    
    return train_transforms, data_transforms

def create_model(model_type, num_classes, device):
    if model_type == 'resnet50':
        model = models.resnet50(weights='IMAGENET1K_V1')
        for param in model.parameters():
            param.requires_grad = True
        model.fc = nn.Linear(model.fc.in_features, num_classes)
    return model.to(device)

def train_epoch(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with tqdm(train_loader, desc='Training') as pbar:
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            pbar.set_postfix({'loss': f'{loss.item():.4f}', 
                            'acc': f'{100.*correct/total:.2f}%'})
    
    return running_loss / len(train_loader), 100. * correct / total

def validate(model, val_loader, criterion, device):
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            val_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    return val_loss / len(val_loader), 100. * correct / total

def get_device():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì ì˜ ë””ë°”ì´ìŠ¤ ì„ íƒ
    1. CUDA GPU
    2. MPS (Apple Silicon)
    3. CPU
    """
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"Using CUDA GPU: {torch.cuda.get_device_name(0)}")
        print(f"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("Using Apple Silicon GPU (MPS)")
    else:
        device = torch.device("cpu")
        print("Using CPU")
    return device

def create_balanced_sampler(dataset):
    """
    í´ëž˜ìŠ¤ ë¶ˆê· í˜•ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ WeightedRandomSampler ìƒì„±
    íŠ¹ížˆ í´ëž˜ìŠ¤ 1ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    """
    targets = [label for _, label in dataset.samples]
    
    # í´ëž˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
    class_count = torch.bincount(torch.tensor(targets))
    print(f"í´ëž˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜: {class_count.tolist()}")
    
    # í´ëž˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚° (ìƒ˜í”Œ ìˆ˜ê°€ ì ì„ìˆ˜ë¡ ê°€ì¤‘ì¹˜ ì¦ê°€)
    class_weights = 1.0 / class_count
    
    # í´ëž˜ìŠ¤ 1ì— ì¶”ê°€ ê°€ì¤‘ì¹˜ ë¶€ì—¬ (ë” ìžì£¼ ìƒ˜í”Œë§ë˜ë„ë¡)
    if len(class_weights) > 1:
        class_weights[1] *= 2.0  # í´ëž˜ìŠ¤ 1ì˜ ê°€ì¤‘ì¹˜ë¥¼ 2ë°°ë¡œ ì¦ê°€
    
    # ê° ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜ ì„¤ì •
    sample_weights = [class_weights[t] for t in targets]
    weights = torch.DoubleTensor(sample_weights)
    
    # WeightedRandomSampler ìƒì„±
    sampler = WeightedRandomSampler(
        weights=weights,
        num_samples=len(weights),
        replacement=True
    )
    
    return sampler

def main():
    args = parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = get_device()
    
    # CUDA ì‚¬ìš© ì‹œ ì¶”ê°€ ì„¤ì •
    if device.type == 'cuda':
        # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        
        # GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥
        print(f"GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1024**2:.2f} MB")
        print(f"GPU Memory Cached: {torch.cuda.memory_reserved(0)/1024**2:.2f} MB")
    
    # ë°ì´í„° ë¡œë” ì„¤ì •
    train_transforms, data_transforms = get_transforms()
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = datasets.ImageFolder(args.train, transform=train_transforms)
    val_dataset = datasets.ImageFolder(args.val, transform=data_transforms)
    test_dataset = datasets.ImageFolder(args.test, transform=data_transforms)
    
    # í´ëž˜ìŠ¤ ë¶ˆê· í˜•ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ sampler ìƒì„±
    train_sampler = create_balanced_sampler(train_dataset)
    
    # í´ëž˜ìŠ¤ ë¶„í¬ ì¶œë ¥
    print(f"í´ëž˜ìŠ¤ ì´ë¦„: {train_dataset.classes}")
    print(f"í´ëž˜ìŠ¤ ì¸ë±ìŠ¤ ë§µí•‘: {train_dataset.class_to_idx}")

    # ë°ì´í„° ë¡œë” ì„¤ì • ì‹œ worker ìˆ˜ ìµœì í™”
    num_workers = 4 if device.type == 'cuda' else 0
    train_loader = DataLoader(train_dataset, 
                            batch_size=args.batch_size,
                            sampler=train_sampler,  # ì»¤ìŠ¤í…€ sampler ì‚¬ìš©
                            num_workers=num_workers,
                            pin_memory=device.type=='cuda')
    val_loader = DataLoader(val_dataset, 
                          batch_size=args.batch_size,
                          shuffle=False,  # ê²€ì¦/í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì…”í”Œí•˜ì§€ ì•ŠìŒ
                          num_workers=num_workers,
                          pin_memory=device.type=='cuda')
    test_loader = DataLoader(test_dataset, 
                           batch_size=args.batch_size,
                           shuffle=False,
                           num_workers=num_workers,
                           pin_memory=device.type=='cuda')
    
    # ëª¨ë¸ ì„¤ì •
    model = create_model(args.model_type, args.num_classes, device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', 
                                                    factor=0.1, patience=3)
    
    # MLflow ì‹¤í—˜ ì‹œìž‘
    with mlflow.start_run():
        mlflow.log_params(vars(args))
        
        best_val_acc = 0.0
        patience_counter = 0
        
        for epoch in range(args.epochs):
            # í›ˆë ¨
            train_loss, train_acc = train_epoch(model, train_loader, 
                                              criterion, optimizer, device)
            
            # ê²€ì¦
            val_loss, val_acc = validate(model, val_loader, criterion, device)
            
            # ë¡œê·¸ ê¸°ë¡
            mlflow.log_metrics({
                'train_loss': train_loss,
                'train_accuracy': train_acc,
                'val_loss': val_loss,
                'val_accuracy': val_acc
            }, step=epoch)
            
            print(f"Epoch {epoch+1}/{args.epochs}")
            print(f"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
            print(f"Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")
            
            # Early Stopping ì²´í¬
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                # ëª¨ë¸ ì €ìž¥
                model_path = os.path.join(args.model_dir, 'model.pth')
                torch.save(model.state_dict(), model_path)
                mlflow.log_artifact(model_path)
            else:
                patience_counter += 1
                if patience_counter >= args.patience:
                    print(f"Early stopping at epoch {epoch+1}")
                    break
            
            scheduler.step(val_acc)
        
        # ìµœì¢… í…ŒìŠ¤íŠ¸
        test_loss, test_acc = validate(model, test_loader, criterion, device)
        print(f"Final Test Accuracy: {test_acc:.2f}%")
        mlflow.log_metric('test_accuracy', test_acc)

if __name__ == '__main__':
    main()